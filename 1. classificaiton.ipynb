{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8e2d2e8",
   "metadata": {},
   "source": [
    "# CS 506 HW1 Solution\n",
    "Name:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d90633",
   "metadata": {},
   "source": [
    "### 1. Understandking K-means Clustering\n",
    "\n",
    "(Please fill out the functions in k_means_clustering.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be82763",
   "metadata": {},
   "source": [
    "### 2. Working with the Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72901e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "# feel free to add more functions such as discard missing examples\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def read_dataset(dataset_path: str):\n",
    "    \"\"\"\n",
    "    read in NYC dataset and return a dataset type of your choice\n",
    "    :param dataset_path: the string path to the dataset file\n",
    "    :return: dataset\n",
    "    \"\"\" \n",
    "    df = pd.read_csv(dataset_path)\n",
    "    df.dropna(subset = ['latitude','longitude','price'])\n",
    "    df = df[['latitude','longitude','price']]\n",
    "    return df\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy.cluster\n",
    "import scipy.cluster.hierarchy as hierarchy\n",
    "from scipy.cluster.hierarchy import centroid, fcluster\n",
    "import scipy.spatial.distance\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "import sklearn.mixture\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scale_dataset():\n",
    "    dataset = read_dataset('listings.csv')\n",
    "    scaler = MinMaxScaler()\n",
    "    dataset[dataset.columns] = scaler.fit_transform(dataset[dataset.columns])\n",
    "    return dataset\n",
    "\n",
    "def cluster_kmeans():\n",
    "    fix_dataset = scale_dataset()\n",
    "    fix_dataset = fix_dataset.iloc[:,:].values   \n",
    "    # determine the number of K, by ploting the graph, error does not decrease\n",
    "    # fast after K = 11, so setting K as 11.\n",
    "    \n",
    "    error = np.zeros(15)\n",
    "    for k in range(1,15):\n",
    "        kmeans = KMeans(init='k-means++', n_clusters = k, n_init = 100)\n",
    "        kmeans.fit_predict(fix_dataset)\n",
    "        error[k] = kmeans.inertia_\n",
    "    \n",
    "    plt.plot(range(1, len(error)), error[1:], 'o-')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.title(r'$k$-means clustering performance')\n",
    "    plt.ylabel('Error');\n",
    "    \n",
    "    kmeans = KMeans(init = 'k-means++', n_clusters = 11, n_init = 100)\n",
    "    k_means = kmeans.fit_predict(fix_dataset)\n",
    "    return k_means\n",
    "\n",
    "def cluster_hierar():        \n",
    "    fix_dataset = scale_dataset()\n",
    "    fix_dataset = fix_dataset.iloc[:,:].values\n",
    "    \n",
    "    # choose cluster as 5 because there are five neighborhoods in the area.\n",
    "    error = np.zeros(7)\n",
    "    for k in range(1,7):\n",
    "        hierar = AgglomerativeClustering(n_clusters = 5, method = 'average')\n",
    "        hierar.fit_predict(fix_dataset)\n",
    "        error[k] = hierar.inertia_\n",
    "    \n",
    "    plt.plot(range(1, len(error)), error[1:], 'o-')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.title(r'$k$-means clustering performance')\n",
    "    plt.ylabel('Error');\n",
    "    \n",
    "    \n",
    "    # Using ward because it is less susceptible to noise and outliers, which probably contains in the dataset\n",
    "    # because some points are far to the the major group.\n",
    "    hierar = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage ='ward')\n",
    "    hierar_labels = hierar.fit_predict(fix_dataset)\n",
    "    return hierar_labels\n",
    "\n",
    "def cluster_GMM():\n",
    "    fix_dataset = scale_dataset()\n",
    "    fix_dataset = fix_dataset.iloc[:,:].values  \n",
    "    \n",
    "    # choose cluster as 5, as the first high bump of Silhouette Score graph is at 5. Covariance_type is full \n",
    "    # because each state has its own general covariance matrix.\n",
    "    \n",
    "    max_clusters = 10\n",
    "    s = np.zeros(max_clusters+1)\n",
    "    for k in range(2, max_clusters+1):\n",
    "        gmm = sklearn.mixture.GaussianMixture(n_components=5, covariance_type='full')\n",
    "        gmm_labels = gmm.fit_predict(fix_dataset)\n",
    "        s[k] = metrics.silhouette_score(fix_dataset, gmm_labels, metric = 'euclidean')\n",
    "    plt.plot(range(2, len(s)), s[2:])\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Silhouette Score');\n",
    "    return s\n",
    "    \n",
    "    gmm = sklearn.mixture.GaussianMixture(n_components=5, covariance_type='full')\n",
    "    gmm_labels = gmm.fit_predict(fix_dataset)\n",
    "    return gmm_labels\n",
    "\n",
    "def cluster_nyc_listings():\n",
    "    \"\"\"\n",
    "    cluster AirBnb listings using k-means++, hierarchical, and GMM\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    return cluster_kmeans(), cluster_hierar(), cluster_GMM()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7645388",
   "metadata": {},
   "source": [
    "### 2b List a few bullet points describing the pros and cons of the various clustering algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f126e3c3",
   "metadata": {},
   "source": [
    "Kmeans:\n",
    "Ad: It guarantees convergence\n",
    "It scales to large data sets.\n",
    "\n",
    "Disad: It tries to find spherical clusters\n",
    "It tries to find equal_sized clusters\n",
    "Good at elliptical graphs\n",
    "It is sensitive to the starting cluster centers\n",
    "\n",
    "Hierarchical:\n",
    "Ad: Easy to decide the number of clusters on the dendrogram\n",
    "Single-linkage clustering can handle non_elliptical shapes\n",
    "\n",
    "Disad: Not suitable for large dataset because of time complexity\n",
    "Initial seeds can be influential to the result\n",
    "Sensitive to outliers\n",
    "\n",
    "GMM:\n",
    "Ad: With GMM, each cluster can have unconstrained covariance structure, so cluster assignment is much more flexible in GMM than in k-means, like elongated or skewed groups can be assigned together.\n",
    "GMM allows for mixed membership of points to clusters. A data does not necessarily belongs to one cluster in the beginning.\n",
    "\n",
    "Disad:\n",
    "It takes a lot of time to run\n",
    "The scaling can be biased sometimes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f37853",
   "metadata": {},
   "source": [
    "### 3 Data Visualization\n",
    "### 3a Produce a Heatmap. Is this heatmap useful in order to draw conclusions about the expensiveness of areas within NYC? if not, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0abc1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import seaborn as sns\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "df = pd.read_csv('listings.csv')\n",
    "\n",
    "def generate_base_map(default_location = [40.693943, -73.985880]):\n",
    "    base_map = folium.Map(location=default_location)\n",
    "    return base_map\n",
    "                      \n",
    "base_map = generate_base_map()\n",
    "HeatMap(\n",
    "    data=df[[\"latitude\", \"longitude\", \"price\"]]\n",
    "    .groupby([\"latitude\", \"longitude\"])\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .values.tolist(),\n",
    "    radius=8,\n",
    "    max_zoom=13,\n",
    ").add_to(base_map)\n",
    "base_map.save(\"index.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3adad9",
   "metadata": {},
   "source": [
    "Yes but not strong, because the dataset are group by its latitude and longitude, so the price level is somewhat reflective on the 2-D map. Although the color does not vary that much, there is still a clear concentrated area in middle part of Manhattan, which is true in real situation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c43ffc",
   "metadata": {},
   "source": [
    "### 3b Visualize the clusters by plotting the longitude / lattitude of every listings in a scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a121b13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade47311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def visualize_kmeans_clusters():\n",
    "    df = read_dataset('listings.csv')\n",
    "    df_xy = df[['longitude','latitude']]\n",
    "    clusters = cluster_kmeans()\n",
    "    longitude = df[['longitude']].to_numpy().reshape(-1)\n",
    "    latitude = df[['latitude']].to_numpy().reshape(-1)\n",
    "    df_final = pd.DataFrame({'longitude': longitude, 'latitude': latitude, 'clusters': clusters})\n",
    "    plot = sns.scatterplot(x = 'longitude', y = 'latitude', data = df_xy, size = 0.0005, hue = clusters)\n",
    "    return plot\n",
    "\n",
    "def visualize_hierar_clusters():\n",
    "    df = read_dataset('listings.csv')\n",
    "    df_xy = df[['longitude','latitude']]\n",
    "    clusters = cluster_hierar()\n",
    "    longitude = df[['longitude']].to_numpy().reshape(-1)\n",
    "    latitude = df[['latitude']].to_numpy().reshape(-1)\n",
    "    df_final = pd.DataFrame({'longitude': longitude, 'latitude': latitude, 'clusters': clusters})\n",
    "    plot = sns.scatterplot(x = 'longitude', y = 'latitude', data = df_xy, size = 0.0005, hue = clusters)\n",
    "    return plot\n",
    "\n",
    "def visualize_GMM_clusters():\n",
    "    df = read_dataset('listings.csv')\n",
    "    df_xy = df[['longitude','latitude']]\n",
    "    clusters = cluster_GMM()\n",
    "    longitude = df[['longitude']].to_numpy().reshape(-1)\n",
    "    latitude = df[['latitude']].to_numpy().reshape(-1)\n",
    "    df_final = pd.DataFrame({'longitude': longitude, 'latitude': latitude, 'clusters': clusters})\n",
    "    plot = sns.scatterplot(x = 'longitude', y = 'latitude', data = df_xy, size = 0.0005, hue = clusters)\n",
    "    return plot\n",
    "\n",
    "def visualize_clusters():\n",
    "    visualize_kmeans_clusters(), visualize_hierar_clusters(), visualize_GMM_clusters()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41991d06",
   "metadata": {},
   "source": [
    "### 3c For every cluster, report the average price of the listings within this cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc0d268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_price_kmeans():\n",
    "    dataset = read_dataset('listings.csv')\n",
    "    fix_dataset = dataset.iloc[:,:].values\n",
    "    cluster = cluster_kmeans()\n",
    "\n",
    "    total = [0] * 11\n",
    "    num = [0] * 11\n",
    "    avg = [0] * 11\n",
    "    label = 0\n",
    "    col_price_list = dataset['price'].tolist()\n",
    "    for index in range(len(cluster)):\n",
    "        label = cluster[index]\n",
    "        total[label] = total[label] + col_price_list[index]\n",
    "        num[label] = num[label] + 1\n",
    "    for i in range(len(total)):\n",
    "        avg[i] = total[i] / num[i]\n",
    "    return avg\n",
    "\n",
    "def average_price_hierar():\n",
    "    dataset = read_dataset('listings.csv')\n",
    "    fix_dataset = dataset.iloc[:,:].values\n",
    "    cluster = cluster_hierar()\n",
    "\n",
    "    total = [0] * 5\n",
    "    num = [0] * 5\n",
    "    avg = [0] * 5\n",
    "    label = 0\n",
    "    col_price_list = dataset['price'].tolist()\n",
    "    for index in range(len(cluster)):\n",
    "        label = cluster[index]\n",
    "        total[label] = total[label] + col_price_list[index]\n",
    "        num[label] = num[label] + 1\n",
    "    for i in range(len(total)):\n",
    "        avg[i] = total[i] / num[i]\n",
    "    return avg\n",
    "\n",
    "def average_price_GMM():\n",
    "    dataset = read_dataset('listings.csv')\n",
    "    fix_dataset = dataset.iloc[:,:].values\n",
    "    cluster = cluster_GMM()\n",
    "\n",
    "    total = [0] * 5\n",
    "    num = [0] * 5\n",
    "    avg = [0] * 5\n",
    "    label = 0\n",
    "    col_price_list = dataset['price'].tolist()\n",
    "    for index in range(len(cluster)):\n",
    "        label = cluster[index]\n",
    "        total[label] = total[label] + col_price_list[index]\n",
    "        num[label] = num[label] + 1\n",
    "    for i in range(len(total)):\n",
    "        avg[i] = total[i] / num[i]\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049d6eab",
   "metadata": {},
   "source": [
    "### 3d Bonus point (provide a plot on an actual NYC map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a29add",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87d990f7",
   "metadata": {},
   "source": [
    "### 3e Are the findings in agreement with what you have in mind about the cost of living for neighborhoods in NYC? If you are unfamiliar with NYC, you can consult the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3abef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes. In my map, I found the highest average cost orrurs at Mid-part and down-part of Manhattan, then Brooklyn. ronx has a lower\n",
    "price, which fits the situation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274e80ba",
   "metadata": {},
   "source": [
    "### 4. Image Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c3e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to add more functions here\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import k_means_clustering as kmc\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import scipy.misc\n",
    "import pdb\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import k_means_clustering as kmc\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import scipy.misc\n",
    "import pdb\n",
    "\n",
    "def cluster_k_means(image_path:str):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = img.reshape((850 * 1280, 3))    \n",
    "    clusters, centroid_history = kmc.run_k_means(img, kmc.choose_random_centroids(img, K=10), n_iter=10)\n",
    "    new_img = img.copy()\n",
    "    for i in range(len(clusters)):\n",
    "        new_img[i] = centroid_history[-1][clusters[i]]\n",
    "\n",
    "    final_img = np.array(new_img)\n",
    "    img = final_img.reshape((850, 1280, 3))\n",
    "    return img\n",
    "\n",
    "def show_image(image):\n",
    "    cv2.imshow('Display Window', image) \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
